AIモデルのアーキテクチャと入出力仕様
このドキュメントは、初期学習で使用されたTransformerベースの麻雀AIモデルの構造、およびその入出力データの仕様について記述する。強化学習環境は、この仕様に準拠したデータを生成する必要がある。

1. 概要
フレームワーク: TensorFlow (Keras)

基本戦略: あるゲーム状況（コンテキスト）と、その状況で取りうる行動のリスト（選択肢）を入力として受け取り、それぞれの選択肢がどれだけ「良い」かをスコアとして出力する。

2. 主要なパラメータ
vectorizer.py で定義されている、モデルの骨格をなす重要な定数。

VECTOR_DIM = 100:

ゲーム内のあらゆる情報（イベント、選択肢）を表現するためのベクトルの次元数。

MAX_CONTEXT_LENGTH = 150:

# AIモデルのアーキテクチャと入出力仕様

このドキュメントは、初期学習で使用された Transformer ベースの麻雀AIモデルの構造と、入出力データ仕様をまとめたものです。強化学習環境は本仕様に準拠したデータを生成する必要があります。

## 1. 概要

- フレームワーク: TensorFlow (Keras)
- 基本戦略: ゲーム状況（コンテキスト）と、その状況で取りうる行動リスト（選択肢）を入力として受け取り、各選択肢の良さをスコア（logit）で出力する。

## 2. 主要な定数（vectorizer.py 由来）

- VECTOR_DIM = 100
	- 各イベント・選択肢を表現するベクトル次元数。
- MAX_CONTEXT_LENGTH = 150
	- 過去イベントとして考慮する最大数（古いイベントは切り捨て）。
- MAX_CHOICES = 50
	- 1 回の判断で考慮する最大選択肢数。

## 3. モデルアーキテクチャ（train_transformer.py ベース）

モデルは主に 3 つの入力を受け取り、(バッチサイズ, MAX_CHOICES) 形状のスコアを出力します。

- コンテキスト入力 (Context Input)
	- 形状: (batch_size, 150, 100)
	- 説明: 過去 150 個のゲームイベントを 100 次元ベクトルで表現した系列。

- 選択肢入力 (Choices Input)
	- 形状: (batch_size, 50, 100)
	- 説明: 最大 50 個の行動選択肢をそれぞれ 100 次元で表現。

- マスク入力 (Mask Input)
	- 形状: (batch_size, 50)
	- 説明: 実際の選択肢が 50 未満のときにダミー選択肢を除外するためのマスク。

処理の流れ:

1. コンテキスト入力を Transformer Encoder（2 ブロック、MultiHeadAttention 4 ヘッドなど）に通し、現在のゲーム状況を要約するベクトルを得る。
2. 要約されたコンテキストベクトルと各選択肢ベクトルを結合。
3. 結合ベクトルを Dense レイヤー等に通し、各選択肢に対する単一のスコア（logit）を出力。
4. 最終出力は形状 (batch_size, 50) のスコア配列。

## 4. 入力データのベクトル化仕様（vectorizer.py 由来）

以下では、各イベント・選択肢が 100 次元ベクトルにどのように格納されるかを示します。

### 4.1 コンテキスト・ベクトル (vectorize_event)

各イベント（例: ツモ、打牌、局開始 など）は 100 次元ベクトルに変換されます。主要なインデックス（意味のあるフィールド）を抜粋します。

| インデックス | 内容 | 正規化 / 説明 |
|---:|---|---|
| vec[0] | イベントID | INIT, DRAW, DISCARD などを数値化 |
| vec[1] | 相対プレイヤーID | 自分を 0、他家を時計回りに 1,2,3 |
| vec[2] | 巡目 | 30.0 で除算して正規化 |
| vec[3-6] | 各プレイヤーの点数 | (点数 - 25000) / 10000 で正規化（自分基準で順に） |
| vec[7] | リーチ牌の数 | - |
| vec[8] | 残りツモ回数 | 70.0 で除算して正規化 |
| vec[10] | 牌ID | イベントに関連する牌の ID（0–135） |
| vec[11] | 手出しかツモ切りか | 打牌イベント時のみ。手出しなら 1.0 |
| vec[12] | ドラ表示牌ID | INIT イベント時のみ |

※ 上記は抜粋です。実際の vectorizer.py を参照して全フィールドを確認してください。

### 4.2 選択肢ベクトル (vectorize_choice)

各選択肢（例: 「5萬を捨てる」「リーチ」など）は 100 次元ベクトルに変換されます。主要フィールドの一例を示します。

| 条件 / 例 | vec[0] | vec[1] | vec[2] | vec[3] |
|---|---:|---:|---:|---:|
| 打牌 (DISCARD_8) | 1.0 | 牌ID (8) | - | - |
| アクション (ACTION_...) | 2.0 | アクションID | 詳細1 | 詳細2 |

アクション ID の例:

- TSUMO: 1
- RON: 2
- RIICHI: 3
- PUNG (ポン): 4
- CHII (チー): 5
- ...

詳細の格納例:

- RIICHI_24 の場合: vec[2] に打牌 ID 24 を入れる。
- CHII_8_12 の場合: vec[2] に 8、vec[3] に 12 を入れる。

（実装の詳細は `vectorize_choice` の実装を参照してください。）

## 5. 出力

- 形式: 各選択肢に対するスコア（logit）の配列。

利用方法:

- 学習時: 正解行動のインデックスをラベルとして SparseCategoricalCrossentropy 等で学習。
- 推論時: 出力スコアに Softmax を適用して確率化し、最も確率の高い選択肢を選択。

## 6. 強化学習環境への示唆

- 環境 (Environment) の役割: ゲームを進行させ、このドキュメントに記載された形式と完全に一致するイベント履歴（context）と選択肢リスト（choices）をエージェントに提供する必要があります。
- エージェント (Agent) の役割: 環境から受け取った情報を `vectorizer.py` の仕様に従ってベクトル化し、モデルに入力、出力のスコアを確率に変換して行動決定を行います。

実装メモ: `transformer_parser.py` と `vectorizer.py` のロジックを流用・統合することが基本要件です。
