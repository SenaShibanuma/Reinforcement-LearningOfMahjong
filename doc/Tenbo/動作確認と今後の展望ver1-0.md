# 開発状況レポートと今後の展望 (Ver. 2.0)

## 1. はじめに

初期のデバッグフェーズを完了し、本プロジェクトは強化学習のサイクル（自己対戦 → データ収集 → モデル更新）が安定して稼働する段階に到達しました。エラーによる中断は解消され、AIは半荘を最後まで打ち切り、学習し、新しいバージョンのモデルを生成し続けることが可能です。

これに伴い、プロジェクトは「動作させる」段階から、「AIを強化し、その性能を分析・改善する」という新たなフェーズに移行します。本ドキュメントは、現在達成済みの機能と、今後の開発計画をまとめたものです。

## 2. 現状の達成事項

### 安定した強化学習サイクル

AIエージェント同士が半荘を最後までプレイし、エラーなく学習・モデル保存を行う一連のプロセスが確立されました。

### 堅牢なバージョン管理

堅牢なバージョン管理システムにより、学習の再開や設定変更に柔軟に対応できます。

### 高度な報酬設計

局所的な最適解を学習するための中間報酬（シャンテン数の進行、聴牌、アガリボーナス）を導入済みです。

大局的な戦略を学習させるため、半荘終了時の**最終順位に応じた報酬（順位点）**を導入済みです。

「アガリなしのゲーム」では順位点を減額するなど、特殊なケースにも対応しています。

### 分析・可視化ツールの導入

GUI対戦ビューワー (`mahjong_viewer.html`): 保存された半荘ログを読み込み、一手ずつ対戦内容を視覚的に再生・分析できます。

TensorBoard: 学習の進捗（損失関数の推移など）をグラフでリアルタイムに可視化し、AIの成長を長期的に追跡できます。

## 3. 今後の展望

AIのさらなる強化と開発効率の向上を目指し、以下の4つの柱で開発を進めます。

### 3.1. AIのさらなる強化

#### アルゴリズムの改良

経験再生 (Experience Replay) の導入: 現在の「1ゲームごとに経験を破棄する」方式から、複数ゲームにわたる大量の経験をバッファに蓄積し、そこからランダムサンプリングして学習する方式に移行します。これにより、学習の安定性とデータ効率が飛躍的に向上します。

探索戦略の高度化: 現在の確率に基づく行動選択から、AlphaGoなどで用いられる高度な探索アルゴリズム（例: UCB1, PUCT）の導入を検討し、より最適な一手を見つけ出す能力を強化します。

#### モデルアーキテクチャの改善

Transformerの層を深くする、ヘッドの数を増やすなどの実験を行い、モデルの表現力向上を目指します。

### 3.2. 学習効率の最大化

#### 並列化による学習高速化 (Actor-Learnerモデル)

学習プロセスを、複数のCPUコアで並行して自己対戦を行うActor（実行役）と、GPUで効率的にモデル更新を行う単一のLearner（学習役）に分離します。これにより、GPUを常にフル稼働させ、学習速度を大幅に向上させます。

#### 事前学習パイプラインの構築

大量の天鳳ログなど、人間の棋譜から教師あり学習を行う「事前学習」のプロセスを正式にパイプライン化します。これにより、ゼロから新しいモデルを育成する際の初期性能を大幅に引き上げることができます。

### 3.3. 評価・分析機能の拡充

#### レーティングシステムの構築

定期的に新旧モデル同士を総当たりで対戦させ、その勝敗からEloレーティングを算出する仕組みを構築します。これにより、AIの強さを客観的な数値で評価できるようになります。

#### 統計データの拡充と可視化

リーチ成功率、放銃率、副露（フーロ）率、平均和了（アガリ）点など、より詳細な統計データを収集します。

収集した統計データをTensorBoardで可視化し、AIの打牌傾向や弱点を多角的に分析します。

#### GUIビューワーの機能強化

各プレイヤーの現在のシャンテン数を表示する機能。

聴牌時に待ち牌をハイライトする機能。

AIが各選択肢をどの程度の確率で評価していたかを表示する機能。

### 3.4. 麻雀ルールの拡張

#### 半荘戦への正式対応

現在の東風戦（8局）設定に加え、南場を含む本格的な半荘戦（16局以上）を正式にサポートし、より長期的で複雑な戦略の学習を可能にします。

## 4. まとめ

本プロジェクトは、強固な学習基盤の構築を完了しました。今後の開発は、AIの雀力そのものを飛躍的に向上させるアルゴリズムの改良、学習速度の高速化、そしてAIの思考を深く理解するための分析機能の強化に注力していきます。
